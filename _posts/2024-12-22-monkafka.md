---
title: "MonKafka: Building a Kafka Broker from Scratch"
author: cef
date: 2024-12-22
categories: [Technical Writing, Open Source]
tags: [Apache Kafka, Open Source]
render_with_liquid: false
description: An attempt to implement a basic Kafka broker.
---

<a href="https://github.com/CefBoud/monkafka" target="_blank">MonKafka</a> is a simple implementation of a basic Kafka broker.  In this post, I will walk through the first steps in creating a minimalistic Kafka broker

# Intro
A few weeks ago, I wrote a [blog post](https://cefboud.com/posts/exploring-kafka-internals/) about my modest exploration of the Kafka codebase. The code diving has been very fun, and I've learned a lot along the wayâ€”especially about networking and concurrency, with a Java flavor. 

Alas, my newfound knowledge didn't quite satisfy my curiosity. On the contrary, it sparked a desire to dig even deeper. Why not implement a mini Kafka broker? After all, as the famous Richard Feynman quote goes, *"What I cannot create, I do not understand."* I try to console myself with this to justify the embarrassing amount of time spent on this project.

Anyway, I chose Golang because it lends itself well to concurrency and networking. I've also started contributing to a Go project recently, ArgoCD, and really enjoyed the simplicity and power of the language. The current state of my tiny Kafka implementation is... not the best. But for now, I decided to focus on building something very basic that works, and iterate from there. Too much overthinking leads to the infamous analysis paralysis.

There is an amazing Kafka implementation in Go called [Jocko](https://github.com/travisjeffery/jocko/). It's truly spectacular. It deals with an older version of the protocol and is almost full-fledged. I considered forking it and starting from there, but I ultimately chose to start from scratch to feel the pain of every design decision instead of relying on training wheels. I still looked at various parts of that codebase for inspiration.

My initial goal, which I set for myself when I started MonKafka, was simple: a Go program that can handle topic creation, Fetch, and Produce requests. 

A modest goal. But more importantly, doable.

---

# The Outset
So here I was, determined to build my own little broker. How to start? It wasn't immediately obvious. I began by reading the [Kafka Protocol Guide](https://kafka.apache.org/protocol). This guide would prove to be the essential reference for implementing the broker (duh...). But although informative, it didn't really provide a step-by-step guide on how to get a broker up and running.

My second idea was to start a Kafka broker following the [quickstart](https://kafka.apache.org/quickstart) tutorial, then run a topic creation command from the CLI, all while running [tcpdump](https://www.tcpdump.org/) to inspect the network traffic. Roughly, I ran the following:


```
# start tcpdump and listen for all traffic on port 9092 (broker port)
sudo tcpdump -i any -X  port 9092  

cd /path/to/kafka_2.13-3.9.0 
bin/kafka-server-start.sh config/kraft/reconfig-server.properties 
bin/kafka-topics.sh --create --topic letsgo  --bootstrap-server localhost:9092
```

The following packets caught my attention (mainly because I saw strings I recognized):

```
16:36:58.121173 IP localhost.64964 > localhost.XmlIpcRegSvc: Flags [P.], seq 1:54, ack 1, win 42871, options [nop,nop,TS val 4080601960 ecr 683608179], length 53
	0x0000:  4500 0069 0000 4000 4006 0000 7f00 0001  E..i..@.@.......
	0x0010:  7f00 0001 fdc4 2384 111e 31c5 eeb4 7f56  ......#...1....V
	0x0020:  8018 a777 fe5d 0000 0101 080a f339 0b68  ...w.].......9.h
	0x0030:  28bf 0873 0000 0031 0012 0004 0000 0000  (..s...1........
	0x0040:  000d 6164 6d69 6e63 6c69 656e 742d 3100  ..adminclient-1.
	0x0050:  1261 7061 6368 652d 6b61 666b 612d 6a61  .apache-kafka-ja
	0x0060:  7661 0633 2e39 2e30 00                   va.3.9.0.



16:36:58.166559 IP localhost.XmlIpcRegSvc > localhost.64965: Flags [P.], seq 1:580, ack 54, win 46947, options [nop,nop,TS val 3149280975 ecr 4098971715], length 579
	0x0000:  4500 0277 0000 4000 4006 0000 7f00 0001  E..w..@.@.......
	0x0010:  7f00 0001 2384 fdc5 3e63 0472 12ab f52e  ....#...>c.r....
	0x0020:  8018 b763 006c 0000 0101 080a bbb6 36cf  ...c.l........6.
	0x0030:  f451 5843 0000 023f 0000 0002 0000 3e00  .QXC...?......>.
	0x0040:  0000 0000 0b00 0001 0000 0011 0000 0200  ................
	0x0050:  0000 0a00 0003 0000 000d 0000 0800 0000  ................
	0x0060:  0900 0009 0000 0009 0000 0a00 0000 0600  ................
	0x0070:  000b 0000 0009 0000 0c00 0000 0400 000d  ................
	0x0080:  0000 0005 0000 0e00 0000 0500 000f 0000  ................
	0x0090:  0005 0000 1000 0000 0500 0011 0000 0001  ................
	0x00a0:  0000 1200 0000 0400 0013 0000 0007 0000  ................
	0x00b0:  1400 0000 0600 0015 0000 0002 0000 1600  ................
	0x00c0:  0000 0500 0017 0000 0004 0000 1800 0000  ................
	0x00d0:  0500 0019 0000 0004 0000 1a00 0000 0500  ................
	0x00e0:  001b 0000 0001 0000 1c00 0000 0400 001d  ................
	0x00f0:  0000 0003 0000 1e00 0000 0300 001f 0000  ................
	0x0100:  0003 0000 2000 0000 0400 0021 0000 0002  ...........!....
	0x0110:  0000 2200 0000 0200 0023 0000 0004 0000  .."......#......
	0x0120:  2400 0000 0200 0025 0000 0003 0000 2600  $......%......&.
	0x0130:  0000 0300 0027 0000 0002 0000 2800 0000  .....'......(...
	0x0140:  0200 0029 0000 0003 0000 2a00 0000 0200  ...)......*.....
	0x0150:  002b 0000 0002 0000 2c00 0000 0100 002d  .+......,......-
	0x0160:  0000 0000 0000 2e00 0000 0000 002f 0000  ............./..
	0x0170:  0000 0000 3000 0000 0100 0031 0000 0001  ....0......1....
	0x0180:  0000 3200 0000 0000 0033 0000 0000 0000  ..2......3......
	0x0190:  3700 0000 0200 0039 0000 0002 0000 3c00  7......9......<.
	0x01a0:  0000 0100 003d 0000 0000 0000 4000 0000  .....=......@...
	0x01b0:  0000 0041 0000 0000 0000 4200 0000 0100  ...A......B.....
	0x01c0:  0044 0000 0001 0000 4500 0000 0000 004a  .D......E......J
	0x01d0:  0000 0000 0000 4b00 0000 0000 0050 0000  ......K......P..
	0x01e0:  0000 0000 5100 0000 0000 0000 0000 0300  ....Q...........
	0x01f0:  3d04 0e67 726f 7570 2e76 6572 7369 6f6e  =..group.version
	0x0200:  0000 0001 000e 6b72 6166 742e 7665 7273  ......kraft.vers
	0x0210:  696f 6e00 0000 0100 116d 6574 6164 6174  ion......metadat
	0x0220:  612e 7665 7273 696f 6e00 0100 1600 0108  a.version.......
	0x0230:  0000 0000 0000 01b0 023d 040e 6772 6f75  .........=..grou
	0x0240:  702e 7665 7273 696f 6e00 0100 0100 0e6b  p.version......k
	0x0250:  7261 6674 2e76 6572 7369 6f6e 0001 0001  raft.version....
	0x0260:  0011 6d65 7461 6461 7461 2e76 6572 7369  ..metadata.versi
	0x0270:  6f6e 0016 0016 00                        on.....

16:36:58.167767 IP localhost.64965 > localhost.XmlIpcRegSvc: Flags [P.], seq 54:105, ack 580, win 42799, options [nop,nop,TS val 4098971717 ecr 3149280975], length 51
	0x0000:  4500 0067 0000 4000 4006 0000 7f00 0001  E..g..@.@.......
	0x0010:  7f00 0001 fdc5 2384 12ab f52e 3e63 06b5  ......#.....>c..
	0x0020:  8018 a72f fe5b 0000 0101 080a f451 5845  .../.[.......QXE
	0x0030:  bbb6 36cf 0000 002f 0013 0007 0000 0003  ..6..../........
	0x0040:  000d 6164 6d69 6e63 6c69 656e 742d 3100  ..adminclient-1.
	0x0050:  0207 6c65 7473 676f ffff ffff ffff 0101  ..letsgo........
	0x0060:  0000 0075 2d00 00     
```

I spotted `adminclient-1`, `group.version`, and `letsgo` (the name of the topic). This looked very promising. Seeing these strings felt like my first win. I thought to myself: so it's not that complicated, it's pretty much about sending the necessary information in an agreed-upon format, i.e., the protocol.

My next goal was to find a request from the CLI client and try to map it to the format described by the protocol. More precisely, figuring out the [request header](https://kafka.apache.org/protocol#protocol_messages):

```
Request Header v2 => request_api_key request_api_version correlation_id client_id TAG_BUFFER 
  request_api_key => INT16
  request_api_version => INT16
  correlation_id => INT32
  client_id => NULLABLE_STRING
```

The `client_id` was my Rosetta stone. I knew its value was equal to `adminclient-1`. At first, because it was kind of common sense. But the proper way is to set the CLI logging level to `DEBUG` by replacing `WARN` in `/path/to/kafka_X.XX-X.X.X/config/tools-log4j.properties`'s `log4j.rootLogger`. At this verbosity level, running the CLI would display `DEBUG [AdminClient clientId=adminclient-1]`, thus removing any doubt about the client ID. This seems somewhat silly, but there are possibly a multitude of candidates for this value: client ID, group ID, instance ID, etc. Better to be sure.

So I found a way to determine the end of the request header: `client_id`. 

```
16:36:58.167767 IP localhost.64965 > localhost.XmlIpcRegSvc: Flags [P.], seq 54:105, ack 580, win 42799, options [nop,nop,TS val 4098971717 ecr 3149280975], length 51
	0x0000:  4500 0067 0000 4000 4006 0000 7f00 0001  E..g..@.@.......
	0x0010:  7f00 0001 fdc5 2384 12ab f52e 3e63 06b5  ......#.....>c..
	0x0020:  8018 a72f fe5b 0000 0101 080a f451 5845  .../.[.......QXE
	0x0030:  bbb6 36cf 0000 002f 0013 0007 0000 0003  ..6..../........
	0x0040:  000d 6164 6d69 6e63 6c69 656e 742d 3100  ..adminclient-1.
	0x0050:  0207 6c65 7473 676f ffff ffff ffff 0101  ..letsgo........
	0x0060:  0000 0075 2d00 00   

```

This nice packet had the `client_id`, but also the topic name. What request could it be? I was naive enough to assume it was for sure the [CreateTopic](https://kafka.apache.org/protocol#The_Messages_CreateTopics) request, but there were other candidates, such as the [Metadata](https://kafka.apache.org/protocol#The_Messages_Metadata), and that assumption was time-consuming.


So `client_id` is a `NULLABLE_STRING`, and per the protocol guide: **first the length N is given as an INT16. Then N bytes follow, which are the UTF-8 encoding of the character sequence**.

Let's remember that in this HEX (base 16) format, a byte (8 bits) is represented using 2 characters from 0 to F. `10` is 16, `ff` is 255, etc.

The line `000d 6164 6d69 6e63 6c69 656e 742d 3100  ..adminclient-1.` is the `client_id` nullable string preceded by its length on two bytes `000d`, meaning 13, and `adminclient-1` has indeed a length equal to 13. As per our spec, the preceding 4 bytes are the `correlation_id` (a unique ID to correlate between requests and responses, since a client can send multiple requests: produce, fetch, metadata, etc.). Its value is `0000 0003`, meaning 3. The 2 bytes preceding it are the `request_api_version`, which is `0007`, i.e. 7, and finally, the 2 bytes preceding that represent the `request_api_key`, which is `0013`, mapping to 19 in decimal. So this is a request whose API key is 19 and its version is 7. And guess what the API key 19 maps to? [`CreateTopic`](https://kafka.apache.org/protocol#The_Messages_CreateTopics)!

This was it. A header, having the API key 19, so the broker knows this is a CreateTopic request and parses it according to its schema. Each version has its own schema, and version 7 looks like the following:

```
CreateTopics Request (Version: 7) => [topics] timeout_ms validate_only TAG_BUFFER 
  topics => name num_partitions replication_factor [assignments] [configs] TAG_BUFFER 
    name => COMPACT_STRING
    num_partitions => INT32
    replication_factor => INT16
    assignments => partition_index [broker_ids] TAG_BUFFER 
      partition_index => INT32
      broker_ids => INT32
    configs => name value TAG_BUFFER 
      name => COMPACT_STRING
      value => COMPACT_NULLABLE_STRING
  timeout_ms => INT32
  validate_only => BOOLEAN
```

We can see the request can have multiple topics because of the `[topics]` field, which is an array. How are arrays encoded in the Kafka protocol? Guide to the rescue:
```
COMPACT_ARRAY :
Represents a sequence of objects of a given type T. 
Type T can be either a primitive type (e.g. STRING) or a structure. 
First, the length N + 1 is given as an UNSIGNED_VARINT. Then N instances of type T follow. 
A null array is represented with a length of 0. 
In protocol documentation an array of T instances is referred to as [T]. |
```

So the array length + 1 is first written as an UNSIGNED_VARINT (a variable-length integer encoding, where smaller values take less space, which is better than traditional fixed encoding). Our array has 1 element, and 1 + 1 = 2, which will be encoded simply as one byte with a value of 2. And this is what we see in the `tcpdump` output:

```
0x0050:  0207 6c65 7473 676f ffff ffff ffff 0101  ..letsgo........
```
`02` is the length of the topics array. It is followed by `name => COMPACT_STRING`, i.e., the encoding of the topic name as a `COMPACT_STRING`, which amounts to the string's length + 1, encoded as a `VARINT`. In our case: `len(letsgo) + 1 = 7`, and we see `07` as the second byte in our `0x0050` line, which is indeed its encoding as a VARINT. After that, we have `6c65 7473 676f` converted to decimal `108 101 116 115 103 111`, which, with UTF-8 encoding, spells `letsgo`.

Let's note that compact strings use varints, and their length is encoded as N+1. This is different from NULLABLE_STRING (like the header's `client_id`), whose length is encoded as N using two bytes.

This process continued for a while. But I think you get the idea. It was simply trying to map the bytes to the protocol. Once that was done, I knew what the client expected and thus what the server needed to respond.

# Implementing Topic Creation


Topic creation felt like a natural starting point. Armed with `tcpdump`'s byte capture and the CLI's debug verbosity, I wanted to understand the exact requests involved in topic creation. They occur in the following order:

1. `RequestApiKey: 18` - APIVersion
2. `RequestApiKey: 3` - Metadata
3. `RequestApiKey: 10` - CreateTopic

The first request, **APIVersion**, is used to ensure compatibility between Kafka clients and servers. The client sends an `APIVersion` request, and the server responds with a list of supported API requests, including their minimum and maximum supported versions.

```
ApiVersions Response (Version: 4) => error_code [api_keys] throttle_time_ms TAG_BUFFER 
  error_code => INT16
  api_keys => api_key min_version max_version TAG_BUFFER 
    api_key => INT16
    min_version => INT16
    max_version => INT16
  throttle_time_ms => INT32
```

An example response might look like this:

```go
APIVersions := types.APIVersionsResponse{
    ErrorCode: 0,
    ApiKeys: []types.APIKey{
        {ApiKey: ProduceKey, MinVersion: 0, MaxVersion: 11},
        {ApiKey: FetchKey, MinVersion: 12, MaxVersion: 12},
        {ApiKey: MetadataKey, MinVersion: 0, MaxVersion: 12},
        {ApiKey: OffsetFetchKey, MinVersion: 0, MaxVersion: 9},
        {ApiKey: FindCoordinatorKey, MinVersion: 0, MaxVersion: 6},
        {ApiKey: JoinGroupKey, MinVersion: 0, MaxVersion: 9},
        {ApiKey: HeartbeatKey, MinVersion: 0, MaxVersion: 4},
        {ApiKey: SyncGroupKey, MinVersion: 0, MaxVersion: 5},
        {ApiKey: APIVersionKey, MinVersion: 0, MaxVersion: 4},
        {ApiKey: CreateTopicKey, MinVersion: 0, MaxVersion: 7},
        {ApiKey: InitProducerIdKey, MinVersion: 0, MaxVersion: 5},
    },
    throttleTimeMs: 0,
}
```

If the client's supported versions do not fall within the `[MinVersion, MaxVersion]` range, there's an incompatibility. 

Once the client sends the `APIVersion` request, it checks the server's response for compatibility. If they are compatible, the client proceeds to the next step. The client sends a **Metadata** request to retrieve information about the brokers and the cluster. The CLI debug log for this request looks like this:

```
DEBUG [AdminClient clientId=adminclient-1] Sending MetadataRequestData(topics=[], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to localhost:9092 (id: -1 rack: null). correlationId=1, timeoutMs=29886 (org.apache.kafka.clients.admin.KafkaAdminClient)
```

After receiving the metadata, the client proceeds to send a **CreateTopic** request to the broker. The debug log for this request is:

```
[AdminClient clientId=adminclient-1] Sending CREATE_TOPICS request with header RequestHeader(apiKey=CREATE_TOPICS, apiVersion=7, clientId=adminclient-1, correlationId=3, headerVersion=2) and timeout 29997 to node 1: CreateTopicsRequestData(topics=[CreatableTopic(name='letsgo', numPartitions=-1, replicationFactor=-1, assignments=[], configs=[])], timeoutMs=29997, validateOnly=false) (org.apache.kafka.clients.NetworkClient)
```

So our Go broker needs to be able to parse these three types of requests and respond appropriately to let the client know that its requests have been handled. As long as we request the protocol schema for the specified API key version, we'll be all set. In terms of implementation, this translates into a simple Golang TCP server.


	

# A Plain TCP Server
At the end of the day, a Kafka broker is nothing more than a TCP server. It parses the Kafka TCP requests based on the API key, then responds with the protocol-agreed-upon format, either saying a topic was created, giving out some metadata, or responding to a consumer's FETCH request with data it has on its log.

The `main.go` of our broker, simplified, is as follows:

```go
func main() {

	storage.Startup(Config, shutdown)

	listener, err := net.Listen("tcp", ":9092")

	for {
		conn, err := listener.Accept()
		if err != nil {
			log.Printf("Error accepting connection: %v\n", err)
			continue
		}
		go handleConnection(conn)
	}
}

```

How about that `handleConnection`? (Simplified)

```go

func handleConnection(conn net.Conn) {
	for {

        // read request length
		lengthBuffer := make([]byte, 4)
		_, err := io.ReadFull(conn, lengthBuffer)

		length := serde.Encoding.Uint32(lengthBuffer)
		buffer := make([]byte, length+4)
		copy(buffer, lengthBuffer)
		// Read remaining request bytes
		_, err = io.ReadFull(conn, buffer[4:])

        // parse header, especially RequestApiKey
		req := serde.ParseHeader(buffer, connectionAddr)
        // use appropriate request handler based on RequestApiKey (request type)
		response := protocol.APIDispatcher[req.RequestApiKey].Handler(req)

        // write responses
		_, err = conn.Write(response)
	}
}
```

This is the whole idea. I intend on adding a queue to handle things more properly, but it is truly no more than a request/response dance. Eerily similar to a web application. To get a bit philosophical, a lot of complex systems boil down to that. It is kind of refreshing to look at it this way. But the devil is in the details, and getting things to work correctly with good performance is where the complexity and challenge lie. This is only the first step in a marathon of minutiae and careful considerations. But the first step is important, nonetheless.

Let's take a look at `ParseHeader`:

```go
func ParseHeader(buffer []byte, connAddr string) types.Request {
	clientIdLen := Encoding.Uint16(buffer[12:])

	return types.Request{
		Length:            Encoding.Uint32(buffer),
		RequestApiKey:     Encoding.Uint16(buffer[4:]),
		RequestApiVersion: Encoding.Uint16(buffer[6:]),
		CorrelationID:     Encoding.Uint32(buffer[8:]),
		ClientId:          string(buffer[14 : 14+clientIdLen]),
		ConnectionAddress: connAddr,
		Body:              buffer[14+clientIdLen+1:], // + 1 to for empty _tagged_fields
	}
}
```

It is almost an exact translation of the manual steps we described earlier. `RequestApiKey` is a 2-byte integer at position 4, `RequestApiVersion` is a 2-byte integer as well, located at position 6. The `clientId` is a string starting at position 14, whose length is read as a 2-byte integer at position 12. It is so satisfying to see. Notice inside `handleConnection` that `req.RequestApiKey` is used as a key to the `APIDispatcher` map.

```go

var APIDispatcher = map[uint16]struct {
	Name    string
	Handler func(req types.Request) []byte
}{
	ProduceKey:         {Name: "Produce", Handler: getProduceResponse},
	FetchKey:           {Name: "Fetch", Handler: getFetchResponse},
	MetadataKey:        {Name: "Metadata", Handler: getMetadataResponse},
	OffsetFetchKey:     {Name: "OffsetFetch", Handler: getOffsetFetchResponse},
	FindCoordinatorKey: {Name: "FindCoordinator", Handler: getFindCoordinatorResponse},
	JoinGroupKey:       {Name: "JoinGroup", Handler: getJoinGroupResponse},
	HeartbeatKey:       {Name: "Heartbeat", Handler: getHeartbeatResponse},
	SyncGroupKey:       {Name: "SyncGroup", Handler: getSyncGroupResponse},
	APIVersionKey:      {Name: "APIVersion", Handler: getAPIVersionResponse},
	CreateTopicKey:     {Name: "CreateTopic", Handler: getCreateTopicResponse},
	InitProducerIdKey:  {Name: "InitProducerId", Handler: getInitProducerIdResponse},
}
```

Each referenced handler parses the request as per the protocol and return an array of bytes encoded as the response expected by the Kafka client. 

Please note that these are only a subset of the current 81 available api keys (request types).




# Serialization and Deserialization using reflection
When I started working on MonKafka, one of the most tedious tasks was encoding/serializing and decoding/deserializing requests and responses from network bytes to Golang structs and vice versa. I soon realized that I was simply following a simple algorithm to perform these actions. This meant the process was ripe for automation.

The requests/responses are represented as structs that contain either simple types (bool, uint16, uint32, uint64, compact strings, etc.), other structs, or an array of structs or simple types.

Let's take a look at the CreateTopic response. In the protocol guide, it is represented as:

```
CreateTopics Response (Version: 7) => throttle_time_ms [topics] TAG_BUFFER 
  throttle_time_ms => INT32
  topics => name topic_id error_code error_message num_partitions replication_factor [configs] TAG_BUFFER 
    name => COMPACT_STRING
    topic_id => UUID
    error_code => INT16
    error_message => COMPACT_NULLABLE_STRING
    num_partitions => INT32
    replication_factor => INT16
    configs => name value read_only config_source is_sensitive TAG_BUFFER 
      name => COMPACT_STRING
      value => COMPACT_NULLABLE_STRING
      read_only => BOOLEAN
      config_source => INT8
      is_sensitive => BOOLEAN
```

We can convert it to the following Go representation:

```go
// CreateTopicsResponseTopic represents a topic's creation result.
type CreateTopicsResponseTopic struct {
	Name              string `kafka:"CompactString"`
	TopicID           [16]byte
	ErrorCode         uint16
	ErrorMessage      string `kafka:"CompactString"`
	NumPartitions     uint32
	ReplicationFactor uint16
	Configs           []CreateTopicsResponseConfig
}

// CreateTopicsResponseConfig represents a configuration for a topic.
type CreateTopicsResponseConfig struct {
	Name         string
	Value        string
	ReadOnly     bool
	ConfigSource uint8
	IsSensitive  bool
}
```

To serialize the struct, we iterate over its fields in the defined orderâ€”which is paramount, as any small alteration will result in deserialization errorsâ€”and based on the type of the field, we add it to our buffer.

1. **Field is a simple type** (bool, uint16, uint32, uint64, compact strings, etc.): Call the appropriate binary encoding function (PutBool, PutUint16, PutUint32, etc.) based on its type.  
2. **Field is a struct**: Recursively call the encode function on it.  
3. **Field is an array**: Put the array's length into the buffer and iterate over its elements, encoding them depending on their type (struct or simple).

And voilÃ !

The function looks as follows:

```go
// Encode encodes a struct using reflection
func (e *Encoder) Encode(x any) {
	t := reflect.TypeOf(x)
	v := reflect.ValueOf(x)

	for i := 0; i < t.NumField(); i++ {
		field := t.Field(i)
		value := v.Field(i)
		if field.Type.Kind() == reflect.Slice {
			e.PutCompactArrayLen(value.Len())
			if field.Type.Elem().Kind() == reflect.Struct { // Slice of structs
				for j := 0; j < value.Len(); j++ {
					e.Encode(value.Index(j).Interface())
				}
			} else { // Slice of basic types
				for j := 0; j < value.Len(); j++ {
					e.Put(value.Index(j).Interface())
				}
			}
		} else if field.Type.Kind() == reflect.Struct {
			e.Encode(value.Interface())
		} else {
			e.Put(value.Interface())
		}
	}
	e.EndStruct()
}
```

The `Put` function for basic types is simply a `switch` statement based on the type:

```go
func (e *Encoder) Put(i any, a ...any) {
	switch c := i.(type) {
	case bool:
		e.PutBool(c)
	case uint8:
		e.PutInt8(c)
	case uint16:
		e.PutInt16(c)
	case uint32:
		e.PutInt32(c)
	case uint64:
		e.PutInt64(c)
	case int:
		e.PutVarint(c)
	case uint:
		e.PutUvarint(c)
	case string:
		e.PutCompactString(c)
	case []byte:
		e.PutCompactBytes(c)
	case [16]byte:
		e.PutBytes(c[:])
	default:
		log.Panic("Unknown type %T", c)
	}
}
```

The deserialization process is quite similar. Given a target struct, we iterate over its fields and decode bytes from the input buffer depending on the field's type (basic, struct, array).

You have probably noticed that I have an annotation/tag for some fields, e.g., `kafka:"CompactString"`. These are not used for now, but I intend to use them later to handle certain cases, such as the differentiation between `CompactString` and `CompactNullableString`, which is ignored for now.  


---
This is it for now - more to come later. Thanks for making it this far!
# Tagged fields
# Segment and Index files
# Building the state
# Disk flushing / memory / mmap
